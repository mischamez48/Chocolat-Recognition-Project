import os
import torch
from torch.utils.data import Dataset
from PIL import Image
import numpy as np
import pandas as pd
import cv2
from torchvision import transforms
import random
from PIL import ImageEnhance
import unicodedata # For normalizing strings because of "Crème Brûlée" in the dataset
from src.utils import extract_chocolate_regions_improved, inverse_transform
from tqdm import tqdm


#=============================================================================
# BIG PART OF THIS DATASET IS GENERATED BY AI USING CHATGPT
# IN ORDER TO QUICKLY GET TO THE DESIGN AND IMPLEMENTATION STAGE
# AND NOT GET BUGGED DOWN IN THE DATASET CREATION WITH ALL THE DETAILS OF FILE
# FORMATS, PATHS, AND OTHER STUFF
#=============================================================================


# Test dataset (no labels)
class ChocolateTestDataset(Dataset):
    def __init__(self, img_dir, transform=None):
        self.img_dir = img_dir
        self.transform = transform
        
        # Get all image files from the test directory
        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.JPG')]
        
        # Extract IDs from filenames (L1000757.JPG -> 1000757)
        self.img_ids = [img_file.split("L")[1].split('.')[0] for img_file in self.img_files]

    def __len__(self):
        return len(self.img_files)
    
    def __getitem__(self, idx):
        img_name = self.img_files[idx]
        img_id = self.img_ids[idx]
        img_path = os.path.join(self.img_dir, img_name)
        
        # Load image
        image = Image.open(img_path).convert('RGB')
        
        # Apply transformations
        if self.transform:
            image = self.transform(image)
        
        return image, img_id

#=============================================================================
# FOR SEGMENTATION
#=============================================================================

class AugmentedChocolateSegmentationDataset(Dataset):
    """Dataset for chocolate instance segmentation with augmented images"""
    def __init__(self, instances_csv, img_dir, mask_dir, transform=None, mask_transform=None, binary_mode=False, encoding='latin-1', verbose=True):
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.transform = transform
        self.mask_transform = mask_transform
        self.binary_mode = binary_mode
        self.verbose = verbose
        
        # Load instance information -> image_id,mask_filename,chocolate_type,bbox_x,bbox_y,bbox_width,bbox_height
        self.instances_df = pd.read_csv(instances_csv, encoding=encoding)
        
        if verbose:
            print("First 3 rows of instances CSV:")
            print(self.instances_df.head(3))
            print(f"CSV columns: {self.instances_df.columns.tolist()}")
        
        # Get unique original image IDs
        self.image_ids = self.instances_df['image_id'].unique()
        self.class_names = sorted(self.instances_df['chocolate_type'].unique())
        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}
        self.idx_to_class = {idx: name for name, idx in self.class_to_idx.items()}
        self.num_actual_classes = len(self.class_names)
        
        # Find all image files in the directory
        self.all_image_files = [f for f in os.listdir(img_dir) 
                              if f.lower().endswith(('.jpg', '.png', '.JPG', '.PNG'))]
        
        # Create a mapping from image IDs to filenames
        self.id_to_img_file = {}
        for img_file in self.all_image_files:
            img_id = os.path.splitext(img_file)[0]  # Remove extension to get ID
            self.id_to_img_file[img_id] = img_file
        
        # Validate which image IDs from CSV exist in train image directory
        self.valid_image_ids = []
        for img_id in self.image_ids:
            if img_id in self.id_to_img_file:
                self.valid_image_ids.append(img_id)
        
        # Get the list of valid image files
        self.valid_img_files = [self.id_to_img_file[img_id] for img_id in self.valid_image_ids]
        
        # Find all mask files
        self.mask_files = {}
        for root, dirs, files in os.walk(mask_dir):
            for file in files:
                if file.endswith('.png'):
                    # Store the full path to the mask file
                    self.mask_files[file] = os.path.join(root, file)
        
        if verbose:
            # Print statistics
            print(f"Loaded {len(self.image_ids)} original images from CSV")
            print(f"Found {len(self.all_image_files)} total image files in directory")
            print(f"Using {len(self.valid_img_files)} valid image files (including augmentations)")
            print(f"Found {len(self.mask_files)} mask files")
            
            # Count originals vs augmentations
            original_count = sum(1 for img in self.valid_img_files if '_' not in os.path.splitext(img)[0])
            augmented_count = len(self.valid_img_files) - original_count
            
            print(f"Original images: {original_count}")
            print(f"Augmented images: {augmented_count}")
            
            #Mostly for debugging purposes
            # Show some examples of valid images and CSV IDs
            if len(self.valid_img_files) > 0:
                print(f"\nFirst {min(5, len(self.valid_img_files))} valid image files:")
                for i, img_file in enumerate(self.valid_img_files[:5]):
                    print(f"  {i+1}. {img_file}")
            
            # Show some mask files
            if self.mask_files:
                print(f"\nFirst {min(5, len(self.mask_files))} mask files:")
                for i, (mask_file, _) in enumerate(list(self.mask_files.items())[:5]):
                    print(f"  {i+1}. {mask_file}")
    
    def normalize_string(self, text): # need to be done because of "Crème Brûlée" in the dataset
        """Remove accents and special characters from a string"""
        # First, decompose the string (separate base characters from accents)
        nfkd_form = unicodedata.normalize('NFKD', text)
        # Remove all the accents (they have the 'Mark' category in Unicode)
        return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])
    
    def __len__(self):
        return len(self.valid_image_ids)
    
    def __getitem__(self, idx):
        # Get image ID and file
        img_id = self.valid_image_ids[idx]
        img_file = self.id_to_img_file[img_id]
        
        # Load image
        img_path = os.path.join(self.img_dir, img_file)
        image = Image.open(img_path).convert('RGB')
        original_size = image.size  # (width, height)
        
        # Get instances for this image from the CSV
        instances = self.instances_df[self.instances_df['image_id'] == img_id]
        
        # Create instance masks and class labels
        masks = []
        labels = []
        boxes = []
        
        for _, instance in instances.iterrows():
            # Get mask filename from CSV
            mask_filename = instance['mask_filename']
            
            # Get full path to mask file
            if mask_filename in self.mask_files:
                mask_path = self.mask_files[mask_filename]
            else:
                # Try to find it in mask_dir/img_id/
                direct_path = os.path.join(self.mask_dir, img_id, mask_filename)
                if os.path.exists(direct_path):
                    mask_path = direct_path
                else:
                    # Skip if mask not found
                    if self.verbose:
                        print(f"Warning: Mask {mask_filename} not found for {img_id}")
                    continue
            
            # Load mask
            mask_np = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            if mask_np is None:
                if self.verbose:
                    print(f"Warning: Could not load mask {mask_path}")
                continue
                
            mask = mask_np > 0
            
            # Get class label
            class_name = instance['chocolate_type']
            
            # Get bounding box
            bbox_x = int(instance['bbox_x'])
            bbox_y = int(instance['bbox_y'])
            bbox_width = int(instance['bbox_width'])
            bbox_height = int(instance['bbox_height'])
            
            # Add to lists
            masks.append(mask)
            labels.append(self.class_to_idx[class_name])
            boxes.append([bbox_x, bbox_y, bbox_x + bbox_width, bbox_y + bbox_height])
        
        # Apply transformations
        if self.transform:
            image = self.transform(image)
        
        # Get transformed dimensions
        if isinstance(image, torch.Tensor):
            _, target_height, target_width = image.shape
        else:
            target_width, target_height = image.size
        
        # Process masks and boxes
        processed_masks = []
        processed_boxes = []
        
        for mask_array, box in zip(masks, boxes):
            # Resize mask to match transformed image
            mask_pil = Image.fromarray((mask_array * 255).astype(np.uint8))
            mask_pil = mask_pil.resize((target_width, target_height), Image.NEAREST)
            processed_mask = np.array(mask_pil) > 0
            processed_masks.append(processed_mask)
            
            # Scale bounding box
            x1, y1, x2, y2 = box
            scale_x = target_width / original_size[0]
            scale_y = target_height / original_size[1]
            
            x1_new = int(x1 * scale_x)
            y1_new = int(y1 * scale_y)
            x2_new = int(x2 * scale_x)
            y2_new = int(y2 * scale_y)
            
            processed_boxes.append([x1_new, y1_new, x2_new, y2_new])
        
        # Convert lists to tensors
        if processed_masks:
            masks_tensor = torch.as_tensor(np.array(processed_masks), dtype=torch.bool)
            boxes_tensor = torch.as_tensor(processed_boxes, dtype=torch.float32)
            labels_tensor = torch.as_tensor(labels, dtype=torch.int64)
        else:
            # Handle case with no masks
            masks_tensor = torch.zeros((0, target_height, target_width), dtype=torch.bool)
            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)
            labels_tensor = torch.zeros(0, dtype=torch.int64)
        
        # Return based on mode
        if self.binary_mode:
            if len(processed_masks) > 0:
                # Combine all instance masks into a single binary mask (logical OR)
                binary_mask = torch.zeros((target_height, target_width), dtype=torch.float32)
                for mask in masks_tensor:
                    binary_mask = torch.logical_or(binary_mask, mask)
                
                # Convert to float for BCE loss compatibility
                binary_mask = binary_mask.float()
                
                return image, binary_mask
            else:
                # Return empty mask if no instances
                return image, torch.zeros((target_height, target_width), dtype=torch.float32)
        else:
            # Create target dictionary for instance segmentation
            target = {
                'boxes': boxes_tensor,
                'labels': labels_tensor,
                'masks': masks_tensor,
                'image_id': torch.tensor([idx]),
                'orig_image_id': img_id
            }
            
            return image, target

class AugmentedChocolateBinarySegmentationDataset(AugmentedChocolateSegmentationDataset):
    """Dataset wrapper for binary segmentation with augmented images"""
    def __init__(self, instances_csv, img_dir, mask_dir, transform=None, mask_transform=None, encoding='latin-1', verbose=True):
        super().__init__(instances_csv, img_dir, mask_dir, transform, mask_transform, binary_mode=True, encoding=encoding, verbose=verbose)



# SegmentationAugmentation class for paired image-mask transformations
class SegmentationAugmentation:
    """
    Advanced augmentation for segmentation tasks that applies consistent
    transformations to both image and mask, preserving the full scene.
    """
    def __init__(self, p_flip=0.5, p_rotate=0.5, p_affine=0.5, p_jitter=0.5, p_elastic=0.3, preserve_full_scene=True):
        """
        Args:
            p_flip: Probability of random flip
            p_rotate: Probability of random rotation
            p_affine: Probability of random affine transformation
            p_jitter: Probability of color jittering
            p_elastic: Probability of elastic transformation
            preserve_full_scene: Whether to preserve the entire scene
        """
        self.p_flip = p_flip
        self.p_rotate = p_rotate
        self.p_affine = p_affine
        self.p_jitter = p_jitter
        self.p_elastic = p_elastic
        self.preserve_full_scene = preserve_full_scene
        
        # Standard normalization for images
        self.normalize = transforms.Normalize(
            mean=[0.6841, 0.6594, 0.6527],
            std=[0.1504, 0.1537, 0.1760]
        )
    
    def __call__(self, image, mask):
        """
        Apply transformations to both image and mask
        
        Args:
            image: PIL Image
            mask: PIL Image (grayscale)
        
        Returns:
            Transformed image tensor and mask tensor
        """
        # Make sure mask is grayscale
        mask = mask.convert('L')
        
        # Apply random horizontal flip
        if random.random() < self.p_flip:
            image = transforms.functional.hflip(image)
            mask = transforms.functional.hflip(mask)
            
        # Apply random vertical flip
        if random.random() < self.p_flip:
            image = transforms.functional.vflip(image)
            mask = transforms.functional.vflip(mask)
        
        # Apply random rotation (with reduced angle for full scene preservation)
        if random.random() < self.p_rotate:
            angle = random.randint(-5, 5)
            image = transforms.functional.rotate(image, angle, fill=245, 
                                              interpolation=transforms.InterpolationMode.BICUBIC)
            mask = transforms.functional.rotate(mask, angle, fill=0)
        
        # Apply color jittering (only to image, not mask)
        if random.random() < self.p_jitter:
            brightness = random.uniform(0.9, 1.1) 
            contrast = random.uniform(0.9, 1.1)    
            saturation = random.uniform(0.9, 1.1)
            
            # Apply color transforms
            image = ImageEnhance.Brightness(image).enhance(brightness)
            image = ImageEnhance.Contrast(image).enhance(contrast)
            image = ImageEnhance.Color(image).enhance(saturation)
        
        # Resize while preserving the full scene
        if self.preserve_full_scene:
            # Get original dimensions
            width, height = image.size
            
            # Calculate target dimensions while preserving aspect ratio
            if width < height:
                new_width = 224
                new_height = int(height * (new_width / width))
            else:
                new_height = 224
                new_width = int(width * (new_height / height))
            
            # Resize both image and mask to these dimensions - no cropping
            image = transforms.functional.resize(image, (new_height, new_width), 
                                             interpolation=transforms.InterpolationMode.BICUBIC)
            mask = transforms.functional.resize(mask, (new_height, new_width), 
                                             interpolation=transforms.InterpolationMode.NEAREST)
        else:
            # Original behavior (direct resize to 224x224)
            image = transforms.functional.resize(image, (224, 224), 
                                             interpolation=transforms.InterpolationMode.BICUBIC)
            mask = transforms.functional.resize(mask, (224, 224), 
                                             interpolation=transforms.InterpolationMode.NEAREST)
        
        # Convert to tensors
        image = transforms.functional.to_tensor(image)
        mask = transforms.functional.to_tensor(mask)
        
        # Apply normalization to image only
        image = self.normalize(image)
        
        return image, mask

def get_segmentation_transforms(train=True):
    """
    Get transforms for segmentation with full scene preservation
    """
    if train:
        #SegmentationAugmentation that preserves the full scene
        return SegmentationAugmentation(
            p_flip=0.5,
            p_rotate=0.3,
            p_affine=0.2,
            p_jitter=0.5,
            preserve_full_scene=True
        )
    else:
        # Simple transform for validation that preserves entire scene
        return transforms.Compose([
            # Just resize the smaller dimension, no cropping
            transforms.Resize(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.6841, 0.6594, 0.6527],
                std=[0.1504, 0.1537, 0.1760]
            )
        ])


#=============================================================================
# FOR CLASSIFICATION
#=============================================================================

class ChocolateFeatureDataset(Dataset):
    """Dataset of chocolate regions with correct labels matched by spatial overlap"""
    def __init__(self, regions, labels, class_names=None, transform=None):
        self.regions = regions
        self.labels = labels
        self.transform = transform

        # Create mapping from class names to indices
        if class_names is None:
            self.class_names = sorted(set(labels))
        else:
            self.class_names = class_names

        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}

    def __len__(self):
        return len(self.regions)

    def __getitem__(self, idx):
        region_image = self.regions[idx]
        label = self.labels[idx]
        class_idx = self.class_to_idx[label]

        # Convert tensor to PIL image for transforms
        if isinstance(region_image, torch.Tensor):
            # De-normalize and convert to PIL
            region_image_pil = transforms.ToPILImage()(inverse_transform(region_image))

            if self.transform:
                region_image = self.transform(region_image_pil)
        else:
            if self.transform:
                region_image = self.transform(region_image)

        return region_image, class_idx
    
def create_aligned_training_data(segmentation_model, device):
    """
    Create a training dataset with correctly labeled chocolate regions
    by matching each region to the corresponding instance mask,
    with improved aspect ratio preservation
    """

    # Load instance-level annotations - use the augmented version
    instances_df = pd.read_csv('dataset_project_iapr2025/train_instances_augmented.csv')

    # Store extracted regions and their correct labels
    extracted_regions = []
    correct_labels = []
    region_images = []
    source_image_ids = []

    # Group instances by image ID
    image_instances = instances_df.groupby('image_id')

    # Process each image
    for image_id, group in tqdm(image_instances, desc="Processing images"):
        # Load image - image_id is already in the correct format (e.g., L1000756_1)
        img_name = f"{image_id}.jpg"
        img_path = os.path.join('dataset_project_iapr2025/train_augmented', img_name)
        
        # Check if file exists with different case extension (.JPG vs .jpg)
        if not os.path.exists(img_path):
            alt_img_path = os.path.join('dataset_project_iapr2025/train_augmented', f"{image_id}.JPG")
            if os.path.exists(alt_img_path):
                img_path = alt_img_path
            else:
                print(f"Warning: Image not found at {img_path} or with .JPG extension")
                continue

        # Load the image
        try:
            image_pil = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {img_path}: {e}")
            continue

        # Convert to tensor using aspect-ratio preserving transform
        # This will resize the smaller dimension to 224 while preserving aspect ratio
        transform = transforms.Compose([
            transforms.Resize(224),  # Resize the smaller dimension to 224
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.6841, 0.6594, 0.6527], std=[0.1504, 0.1537, 0.1760])
        ])

        image_tensor = transform(image_pil)

        # Extract regions using segmentation model with improved aspect ratio preservation
        regions = extract_chocolate_regions_improved(
            image=image_tensor,
            segmentation_model=segmentation_model,
            device=device,
            min_distance=10,
            min_size=300
        )

        if len(regions) == 0:
            continue

        # Load all instance masks for this image
        instance_masks = []
        instance_types = []

        for _, instance in group.iterrows():
            # Load mask
            mask_filename = instance['mask_filename']
            # The masks are now in a directory structure like train_masks_augmented/L1000756_1/L1000756_1_Jelly_White_2.png
            mask_path = os.path.join('dataset_project_iapr2025/train_masks_augmented', image_id, mask_filename)

            if os.path.exists(mask_path):
                # Load mask and resize to match the transformed image dimensions
                try:
                    mask_np = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
                    
                    # Resize mask to match the transformed image dimensions
                    img_h, img_w = image_tensor.shape[1], image_tensor.shape[2]
                    mask_pil = Image.fromarray(mask_np)
                    mask_pil = mask_pil.resize((img_w, img_h), Image.NEAREST)
                    mask = np.array(mask_pil) > 0
                except Exception as e:
                    print(f"Error processing mask {mask_path}: {e}")
                    mask = np.zeros((image_tensor.shape[1], image_tensor.shape[2]), dtype=bool)
            else:
                # If mask doesn't exist, create an empty one with the same dimensions
                print(f"Warning: Mask {mask_path} not found")
                mask = np.zeros((image_tensor.shape[1], image_tensor.shape[2]), dtype=bool)

            instance_masks.append(mask)

            # Get class label
            class_name = instance['chocolate_type']
            instance_types.append(class_name)

        # For each extracted region, find the best matching instance mask
        for region in regions:
            region_mask = region['mask']

            # Find the instance with highest IoU
            best_iou = 0
            best_instance_idx = -1

            for i, instance_mask in enumerate(instance_masks):
                # Calculate IoU
                intersection = np.logical_and(region_mask, instance_mask).sum()
                union = np.logical_or(region_mask, instance_mask).sum()
                iou = intersection / union if union > 0 else 0

                if iou > best_iou:
                    best_iou = iou
                    best_instance_idx = i

            # Only use regions with good match to ground truth
            if best_iou > 0.3:  # Threshold for considering it a match
                # Save the region with aspect ratio preserved
                extracted_regions.append(region)
                correct_labels.append(instance_types[best_instance_idx])
                region_images.append(region['image'])
                source_image_ids.append(image_id)

    print(f"Created dataset with {len(extracted_regions)} correctly labeled regions")

    # Show label distribution
    label_counts = {}
    for label in correct_labels:
        label_counts[label] = label_counts.get(label, 0) + 1

    print("\nLabel distribution:")
    for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {label}: {count} ({count/len(correct_labels)*100:.1f}%)")

    return extracted_regions, correct_labels, region_images, source_image_ids

def get_robust_feature_transforms(train=True, consistency_mode=False):
    """
    Extremely enhanced transforms for bridging the validation-test gap with correct ordering

    Args:
        train: Whether to include training augmentations
        consistency_mode: Whether to use mild augmentation for validation
    """
    # Base transforms that are IDENTICAL for both training and validation
    base_transforms = [
        # Resize preserving aspect ratio but with more space for transformations
        transforms.Lambda(lambda img: resize_with_aspect_ratio(img, 280)),
    ]

    # Normalization is also identical
    normalization = [
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.6841, 0.6594, 0.6527], std=[0.1504, 0.1537, 0.1760])
    ]

    if train:
        pre_tensor_transforms = [
            #orientation invariance
            transforms.RandomRotation(
                180,  # Full rotation range (chocolates can be oriented any way)
                fill=245,
                interpolation=transforms.InterpolationMode.BICUBIC
            ),

            # Random cropping to simulate partial visibility
            transforms.RandomResizedCrop(
                224,
                scale=(0.65, 1.0),  # Allow cropping up to 35% of the image
                ratio=(0.8, 1.25)   # Allow more aspect ratio variation
            ),

            # Standard flips
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),

            #color jitter to simulate different lighting
            transforms.ColorJitter(
                brightness=(0.9, 1.1),  
                contrast=(0.9, 1.1),   
                saturation=(0.9, 1.1), 
                hue=0.05           
            ),


            # Occasionally convert to grayscale for color invariance, might not be that useful
            transforms.RandomGrayscale(p=0.05),

            # Add blur to simulate camera focus issues
            transforms.RandomApply([
                transforms.GaussianBlur(
                    kernel_size=5,
                    sigma=(0.1, 2.0)
                )
            ], p=0.3),
        ]


        # Combine transforms in proper order:
        # 1. PIL transforms first
        # 2. Then convert to tensor & normalize
        # 3. Then tensor transforms
        return transforms.Compose(
            base_transforms +
            pre_tensor_transforms +
            normalization
        )

    elif consistency_mode:
        # Moderately strong augmentations for consistency mode
        mild_augmentations = [
            # Moderate rotation
            transforms.RandomRotation(30, fill=245),

            # Standard flips
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),

            # Moderate color jitter
            transforms.ColorJitter(
                brightness=0.05,
                contrast=0.05,
                saturation=0.05,
                hue=0.05
            ),

            # Center crop to ensure consistent dimensions
            transforms.CenterCrop(224)
        ]

        return transforms.Compose(
            base_transforms +
            mild_augmentations +
            normalization
        )

    else:
        # Standard validation - clean with minimal processing
        validation_transforms = [
            # Center crop to ensure consistent dimensions
            transforms.CenterCrop(224)
        ]

        return transforms.Compose(
            base_transforms +
            validation_transforms +
            normalization
        )

def resize_with_aspect_ratio(img, target_size):
    """
    Resize image while preserving aspect ratio

    Args:
        img: PIL Image
        target_size: Target size for the smaller dimension

    Returns:
        Resized PIL Image with preserved aspect ratio
    """
    # Get original dimensions
    width, height = img.size

    # Calculate target dimensions while preserving aspect ratio
    if width < height:
        # Width is smaller dimension
        new_width = target_size
        new_height = int(height * (new_width / width))
    else:
        # Height is smaller dimension
        new_height = target_size
        new_width = int(width * (new_height / height))

    # Resize image
    return img.resize((new_width, new_height), Image.BICUBIC)